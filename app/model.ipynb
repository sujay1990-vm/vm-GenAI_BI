{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c60f524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting implicit\n",
      "  Downloading implicit-0.7.2-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (1.26.4)\n",
      "Requirement already satisfied: scipy>=0.16 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (1.14.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (4.66.2)\n",
      "Requirement already satisfied: threadpoolctl in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->implicit) (0.4.6)\n",
      "Downloading implicit-0.7.2-cp310-cp310-win_amd64.whl (748 kB)\n",
      "   ---------------------------------------- 748.6/748.6 kB 6.2 MB/s eta 0:00:00\n",
      "Installing collected packages: implicit\n",
      "Successfully installed implicit-0.7.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! py -m pip install implicit  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26b903d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd, numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.cluster import KMeans\n",
    "# import joblib\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 1.  Build (or refresh) clusters offline\n",
    "# # ----------------------------------------\n",
    "# fs   = pd.read_csv(\"feature_store_enhanced.csv\")\n",
    "# num_cols = [\n",
    "#     \"Age\", \"Annual_Income\", \"Total_Spend\", \"Num_Transactions\",\n",
    "#     \"Spend_Grocery\", \"Spend_Travel\", \"Spend_Fuel\",\n",
    "#     \"Spend_Dining\", \"Spend_Education\", \"Salary_to_Spend_Ratio\"\n",
    "# ]\n",
    "# X = fs[num_cols].fillna(0)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_std  = scaler.fit_transform(X)\n",
    "\n",
    "# k = 8                                      # 6–10 clusters works well\n",
    "# km = KMeans(n_clusters=k, random_state=42)\n",
    "# cluster_labels = km.fit_predict(X_std)\n",
    "\n",
    "# save artefacts\n",
    "# fs[\"segment\"] = cluster_labels\n",
    "# fs[[\"Customer_ID\",\"segment\"]].to_csv(\"segment_map.csv\", index=False)\n",
    "# joblib.dump(km,     \"segment_kmeans.joblib\")\n",
    "# joblib.dump(scaler, \"segment_scaler.joblib\")\n",
    "\n",
    "\n",
    "# cust_seg = pd.read_csv(\"segment_map.csv\").set_index(\"Customer_ID\")[\"segment\"]\n",
    "# cust_prod = pd.read_csv(\"customer_products.csv\",\n",
    "#                         parse_dates=[\"Acquisition_Date\"])\n",
    "\n",
    "# takeup = (\n",
    "#     cust_prod\n",
    "#     #   .query(\"Acquisition_Date.between('2024-04-01','2024-06-30')\")\n",
    "#       .merge(cust_seg, on=\"Customer_ID\")\n",
    "#       .groupby([\"segment\",\"Product_ID\"]).size()\n",
    "#       .div(cust_seg.value_counts())        # denominator per cluster\n",
    "#       .unstack(fill_value=0)               # rows = segment, cols = product\n",
    "# )\n",
    "\n",
    "# takeup.to_parquet(\"segment_takeup.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4fbe5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, joblib\n",
    "\n",
    "# Load artefacts once at app start-up\n",
    "scaler = joblib.load(\"segment_scaler.joblib\")\n",
    "kmeans = joblib.load(\"segment_kmeans.joblib\")\n",
    "\n",
    "# Same feature list used during training\n",
    "SEG_FEATS = [\n",
    "    \"Age\", \"Annual_Income\", \"Total_Spend\", \"Num_Transactions\",\n",
    "    \"Spend_Grocery\", \"Spend_Travel\", \"Spend_Fuel\",\n",
    "    \"Spend_Dining\", \"Spend_Education\", \"Salary_to_Spend_Ratio\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab57405",
   "metadata": {},
   "source": [
    "### save new customer in one of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "import json\n",
    "\n",
    "@tool\n",
    "def assign_segment(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Input  ► JSON {\"customer_id\": \"CUST9999\"}\n",
    "    Output ► JSON {\"customer_id\": \"CUST9999\", \"segment\": 3}\n",
    "\n",
    "    • Pulls the customer's feature row\n",
    "    • Applies saved scaler → k-means.predict\n",
    "    • Returns the cluster label (0-7)\n",
    "    \"\"\"\n",
    "    payload = json.loads(input)\n",
    "    cid     = payload[\"customer_id\"]\n",
    "\n",
    "    # 1. fetch row from feature_store\n",
    "    row = pd.read_sql(\n",
    "        f\"SELECT {', '.join(SEG_FEATS)} FROM feature_store \"\n",
    "        f\"WHERE Customer_ID='{cid}'\", conn\n",
    "    )\n",
    "\n",
    "    if row.empty:\n",
    "        return json.dumps({\"customer_id\": cid, \"segment\": None})\n",
    "\n",
    "    # 2. scale & predict\n",
    "    X_std   = scaler.transform(row[SEG_FEATS].fillna(0))\n",
    "    seg_id  = int(kmeans.predict(X_std)[0])\n",
    "\n",
    "    # (optional) append to in-memory mapping for later lookups\n",
    "    seg_map = pd.read_csv(\"segment_map.csv\").set_index(\"Customer_ID\")[\"segment\"]\n",
    "    seg_map[cid] = seg_id       # seg_map is the Series you loaded earlier\n",
    "\n",
    "    return json.dumps({\"customer_id\": cid, \"segment\": seg_id})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c759dd",
   "metadata": {},
   "source": [
    "## batch updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "takeup = (\n",
    "    customer_products\n",
    "      .merge(seg_map, on=\"Customer_ID\")\n",
    "      .groupby([\"segment\",\"Product_ID\"]).size()\n",
    "      .div(seg_map.value_counts())\n",
    "      .unstack(fill_value=0)\n",
    ")\n",
    "takeup.to_parquet(\"segment_takeup.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4ad57",
   "metadata": {},
   "source": [
    "### Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a7c940a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 20 rules to synergy_rules.json\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 0)  Install mlxtend if you haven't yet\n",
    "#      pip install mlxtend\n",
    "# -------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import json\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1)  Load customer–product ownership\n",
    "# ------------------------------------------------------------------\n",
    "cust_prod = pd.read_csv(\"customer_products.csv\")   # or SELECT from DB\n",
    "# Expected columns: Customer_ID, Product_ID, Acquisition_Date, …\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2)  Build a customer×product boolean matrix (one row per customer)\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Build customer × product matrix as Boolean\n",
    "basket = (\n",
    "    cust_prod\n",
    "      .assign(flag=1)\n",
    "      .pivot_table(index=\"Customer_ID\",\n",
    "                   columns=\"Product_ID\",\n",
    "                   values=\"flag\",\n",
    "                   aggfunc=\"max\",\n",
    "                   fill_value=0)\n",
    "      .astype(bool)           # <-- make values True / False\n",
    ")\n",
    "\n",
    "# 3) Frequent itemsets\n",
    "itemsets = apriori(\n",
    "    basket,\n",
    "    min_support=0.03,\n",
    "    use_colnames=True\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4)  Association rules\n",
    "#     - metric = 'lift' gives us the lift value directly\n",
    "# ------------------------------------------------------------------\n",
    "rules = association_rules(\n",
    "    itemsets,\n",
    "    metric=\"lift\",\n",
    "    min_threshold=1.5      # keep only lift ≥ 1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5)  Convert frozensets → list, keep key metrics, and dump to JSON\n",
    "# ------------------------------------------------------------------\n",
    "rules_out = []\n",
    "keep_cols = [\n",
    "    \"antecedent support\", \"consequent support\", \"support\",\n",
    "    \"confidence\", \"lift\", \"leverage\", \"conviction\"\n",
    "]\n",
    "\n",
    "for _, row in rules.iterrows():\n",
    "    rules_out.append({\n",
    "        \"antecedents\": list(row[\"antecedents\"]),\n",
    "        \"consequents\": list(row[\"consequents\"]),\n",
    "        **{k: float(row[k]) for k in keep_cols}\n",
    "    })\n",
    "\n",
    "with open(\"synergy_rules.json\", \"w\") as f:\n",
    "    json.dump(rules_out, f, indent=2)\n",
    "\n",
    "print(f\"✅ Wrote {len(rules_out)} rules to synergy_rules.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd322c",
   "metadata": {},
   "source": [
    "## ALS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad6b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SujaySunilNagvekar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 8 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba3c8aab84e4a54893e3b06a226f7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ALS model & indices saved.\n"
     ]
    }
   ],
   "source": [
    "# pip install implicit        (GPU optional)\n",
    "import pandas as pd, numpy as np, scipy.sparse as sp, joblib\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "# 1) Build customer–product sparse matrix\n",
    "cust_prod = pd.read_csv(\"customer_products.csv\")\n",
    "row_idx   = {cid: i for i, cid in enumerate(cust_prod[\"Customer_ID\"].unique())}\n",
    "col_idx   = {pid: j for j, pid in enumerate(cust_prod[\"Product_ID\"].unique())}\n",
    "\n",
    "rows = cust_prod[\"Customer_ID\"].map(row_idx)\n",
    "cols = cust_prod[\"Product_ID\"].map(col_idx)\n",
    "data = np.ones(len(cust_prod))           # implicit feedback = 1\n",
    "\n",
    "matrix = sp.csr_matrix((data, (rows, cols)),\n",
    "                       shape=(len(row_idx), len(col_idx)))\n",
    "\n",
    "# 2) Train ALS (implicit)\n",
    "als = AlternatingLeastSquares(factors=8,\n",
    "                              regularization=0.1,\n",
    "                              iterations=30,\n",
    "                              random_state=42)\n",
    "als.fit(matrix)\n",
    "\n",
    "# 3) Persist artefacts\n",
    "joblib.dump(als,      \"als_model.joblib\")\n",
    "joblib.dump(row_idx,  \"user_index.joblib\")\n",
    "joblib.dump(col_idx,  \"item_index.joblib\")\n",
    "print(\"✅ ALS model & indices saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7131a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a1802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
