{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!py -m pip install nltk\n",
    "\n",
    "\n",
    "# !py -m pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai langchain-chroma bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                                  Version\n",
      "---------------------------------------- -------------------\n",
      "absl-py                                  1.0.0\n",
      "aiohttp                                  3.9.5\n",
      "aiosignal                                1.3.1\n",
      "altair                                   5.3.0\n",
      "annotated-types                          0.6.0\n",
      "anyio                                    3.5.0\n",
      "argon2-cffi                              21.3.0\n",
      "argon2-cffi-bindings                     21.2.0\n",
      "asgiref                                  3.8.1\n",
      "asttokens                                2.0.5\n",
      "astunparse                               1.6.3\n",
      "async-timeout                            4.0.3\n",
      "asyncio                                  3.4.3\n",
      "attrs                                    21.4.0\n",
      "Babel                                    2.9.1\n",
      "backcall                                 0.2.0\n",
      "backoff                                  2.2.1\n",
      "bcrypt                                   4.1.3\n",
      "beautifulsoup4                           4.12.3\n",
      "black                                    21.12b0\n",
      "bleach                                   4.1.0\n",
      "blinker                                  1.7.0\n",
      "blis                                     1.0.1\n",
      "boto3                                    1.34.112\n",
      "botocore                                 1.34.112\n",
      "bs4                                      0.0.2\n",
      "build                                    1.2.1\n",
      "cachetools                               5.0.0\n",
      "catalogue                                2.0.10\n",
      "certifi                                  2024.2.2\n",
      "cffi                                     1.15.0\n",
      "chardet                                  5.2.0\n",
      "charset-normalizer                       3.3.2\n",
      "chroma-hnswlib                           0.7.3\n",
      "chromadb                                 0.5.0\n",
      "click                                    8.1.7\n",
      "cloudpathlib                             0.20.0\n",
      "cloudpickle                              2.0.0\n",
      "cohere                                   5.5.3\n",
      "colorama                                 0.4.6\n",
      "coloredlogs                              15.0.1\n",
      "comm                                     0.2.2\n",
      "comtypes                                 1.4.5\n",
      "confection                               0.1.5\n",
      "contourpy                                1.3.0\n",
      "cryptography                             43.0.1\n",
      "cycler                                   0.11.0\n",
      "cymem                                    2.0.10\n",
      "dataclasses-json                         0.6.4\n",
      "datasets                                 2.19.1\n",
      "debugpy                                  1.5.1\n",
      "decorator                                5.1.1\n",
      "deepdiff                                 7.0.1\n",
      "defusedxml                               0.7.1\n",
      "Deprecated                               1.2.14\n",
      "dill                                     0.3.8\n",
      "dirtyjson                                1.0.8\n",
      "diskcache                                5.6.3\n",
      "distro                                   1.9.0\n",
      "Django                                   5.0.6\n",
      "dnspython                                2.6.1\n",
      "docopt                                   0.6.2\n",
      "email_validator                          2.1.1\n",
      "emoji                                    2.11.1\n",
      "entrypoints                              0.3\n",
      "et-xmlfile                               1.1.0\n",
      "executing                                0.8.2\n",
      "faiss-cpu                                1.8.0\n",
      "Faker                                    33.1.0\n",
      "fastapi                                  0.111.0\n",
      "fastapi-cli                              0.0.3\n",
      "fastavro                                 1.9.4\n",
      "fastcore                                 1.7.11\n",
      "fastjsonschema                           2.20.0\n",
      "filelock                                 3.14.0\n",
      "filetype                                 1.2.0\n",
      "flatbuffers                              2.0\n",
      "fonttools                                4.28.5\n",
      "frozenlist                               1.4.1\n",
      "fsspec                                   2024.3.1\n",
      "gast                                     0.5.3\n",
      "geopandas                                1.0.1\n",
      "gitdb                                    4.0.11\n",
      "GitPython                                3.1.43\n",
      "gmft                                     0.3.1\n",
      "google-auth                              2.6.0\n",
      "google-auth-oauthlib                     0.4.6\n",
      "google-pasta                             0.2.0\n",
      "googleapis-common-protos                 1.63.0\n",
      "greenlet                                 3.0.3\n",
      "grpcio                                   1.63.0\n",
      "gTTS                                     2.5.1\n",
      "gym                                      0.23.0\n",
      "gym-notices                              0.0.6\n",
      "h11                                      0.14.0\n",
      "h5py                                     3.6.0\n",
      "httpcore                                 1.0.5\n",
      "httptools                                0.6.1\n",
      "httpx                                    0.27.0\n",
      "httpx-sse                                0.4.0\n",
      "huggingface-hub                          0.23.0\n",
      "humanfriendly                            10.0\n",
      "idna                                     3.7\n",
      "imbalanced-learn                         0.9.0\n",
      "importlib-metadata                       7.0.0\n",
      "importlib_resources                      6.4.0\n",
      "intel-openmp                             2021.4.0\n",
      "ipykernel                                6.7.0\n",
      "ipython                                  8.12.3\n",
      "ipython-genutils                         0.2.0\n",
      "ipywidgets                               8.1.3\n",
      "jedi                                     0.18.1\n",
      "Jinja2                                   3.0.3\n",
      "jiter                                    0.6.0\n",
      "jmespath                                 1.0.1\n",
      "joblib                                   1.1.0\n",
      "json5                                    0.9.6\n",
      "jsonpatch                                1.33\n",
      "jsonpath-python                          1.0.6\n",
      "jsonpointer                              2.4\n",
      "jsonschema                               4.4.0\n",
      "jupyter-client                           7.1.1\n",
      "jupyter_core                             5.7.2\n",
      "jupyter-server                           1.13.3\n",
      "jupyterlab                               3.2.8\n",
      "jupyterlab-pygments                      0.1.2\n",
      "jupyterlab-server                        2.10.3\n",
      "jupyterlab_widgets                       3.0.11\n",
      "kagglehub                                0.3.3\n",
      "kaleido                                  0.2.1\n",
      "keras                                    2.8.0\n",
      "Keras-Preprocessing                      1.1.2\n",
      "keyboard                                 0.13.5\n",
      "kiwisolver                               1.3.2\n",
      "kubernetes                               29.0.0\n",
      "langchain                                0.3.14\n",
      "langchain-chroma                         0.1.4\n",
      "langchain-community                      0.3.14\n",
      "langchain-core                           0.3.29\n",
      "langchain-experimental                   0.3.4\n",
      "langchain-openai                         0.2.14\n",
      "langchain-text-splitters                 0.3.4\n",
      "langchainhub                             0.1.20\n",
      "langcodes                                3.5.0\n",
      "langdetect                               1.0.9\n",
      "langgraph                                0.2.60\n",
      "langgraph-checkpoint                     2.0.9\n",
      "langgraph-sdk                            0.1.48\n",
      "langsmith                                0.1.147\n",
      "language_data                            1.3.0\n",
      "libclang                                 13.0.0\n",
      "libsvm                                   3.23.0.4\n",
      "lida                                     0.0.14\n",
      "llama-cloud                              0.1.2\n",
      "llama-index                              0.10.19\n",
      "llama-index-agent-openai                 0.1.7\n",
      "llama-index-cli                          0.1.13\n",
      "llama-index-core                         0.10.19\n",
      "llama-index-embeddings-openai            0.1.11\n",
      "llama-index-indices-managed-llama-cloud  0.1.6\n",
      "llama-index-legacy                       0.9.48.post3\n",
      "llama-index-llms-openai                  0.1.11\n",
      "llama-index-multi-modal-llms-openai      0.1.9\n",
      "llama-index-program-openai               0.1.6\n",
      "llama-index-question-gen-openai          0.1.3\n",
      "llama-index-readers-file                 0.1.22\n",
      "llama-index-readers-llama-parse          0.1.6\n",
      "llama-parse                              0.4.9\n",
      "llamaindex-py-client                     0.1.19\n",
      "llmx                                     0.0.21a0\n",
      "lxml                                     5.2.1\n",
      "marisa-trie                              1.2.1\n",
      "Markdown                                 3.3.6\n",
      "markdown-it-py                           3.0.0\n",
      "MarkupSafe                               2.0.1\n",
      "marshmallow                              3.21.1\n",
      "matplotlib                               3.9.2\n",
      "matplotlib-inline                        0.1.3\n",
      "matplotlib-venn                          1.1.1\n",
      "mdurl                                    0.1.2\n",
      "mistune                                  3.0.2\n",
      "mizani                                   0.13.0\n",
      "mkl                                      2021.4.0\n",
      "mmh3                                     4.1.0\n",
      "monotonic                                1.6\n",
      "mpmath                                   1.3.0\n",
      "msgpack                                  1.1.0\n",
      "multidict                                6.0.5\n",
      "multiprocess                             0.70.16\n",
      "murmurhash                               1.0.11\n",
      "mypy-extensions                          1.0.0\n",
      "nbclassic                                0.3.5\n",
      "nbclient                                 0.5.10\n",
      "nbconvert                                7.16.4\n",
      "nbformat                                 5.10.4\n",
      "neo4j                                    5.27.0\n",
      "neo4j-graphrag                           1.2.1\n",
      "nest-asyncio                             1.6.0\n",
      "networkx                                 3.3\n",
      "nltk                                     3.9.1\n",
      "notebook                                 6.4.7\n",
      "numexpr                                  2.10.0\n",
      "numpy                                    1.26.4\n",
      "oauthlib                                 3.2.2\n",
      "olefile                                  0.47\n",
      "onnxruntime                              1.17.3\n",
      "openai                                   1.59.4\n",
      "opencv-python                            4.10.0.84\n",
      "openpyxl                                 3.0.9\n",
      "opentelemetry-api                        1.24.0\n",
      "opentelemetry-exporter-otlp-proto-common 1.24.0\n",
      "opentelemetry-exporter-otlp-proto-grpc   1.24.0\n",
      "opentelemetry-instrumentation            0.45b0\n",
      "opentelemetry-instrumentation-asgi       0.45b0\n",
      "opentelemetry-instrumentation-fastapi    0.45b0\n",
      "opentelemetry-proto                      1.24.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opentelemetry-sdk                        1.24.0\n",
      "opentelemetry-semantic-conventions       0.45b0\n",
      "opentelemetry-util-http                  0.45b0\n",
      "opt-einsum                               3.3.0\n",
      "ordered-set                              4.1.0\n",
      "orjson                                   3.10.1\n",
      "overrides                                7.7.0\n",
      "packaging                                23.2\n",
      "pandas                                   2.2.3\n",
      "pandocfilters                            1.5.0\n",
      "parso                                    0.8.3\n",
      "pathspec                                 0.9.0\n",
      "patsy                                    0.5.6\n",
      "pdf2image                                1.17.0\n",
      "pickleshare                              0.7.5\n",
      "pillow                                   10.4.0\n",
      "pip                                      24.3.1\n",
      "pipreqs                                  0.5.0\n",
      "platformdirs                             4.3.6\n",
      "playsound                                1.3.0\n",
      "plotly                                   5.19.0\n",
      "plotnine                                 0.14.1\n",
      "posthog                                  3.5.0\n",
      "preshed                                  3.0.9\n",
      "prometheus-client                        0.12.0\n",
      "prompt_toolkit                           3.0.48\n",
      "protobuf                                 3.20.0\n",
      "psutil                                   6.0.0\n",
      "pure-eval                                0.2.1\n",
      "pyarrow                                  16.0.0\n",
      "pyarrow-hotfix                           0.6\n",
      "pyasn1                                   0.4.8\n",
      "pyasn1-modules                           0.2.8\n",
      "PyAudio                                  0.2.14\n",
      "pyclipper                                1.3.0.post5\n",
      "pycparser                                2.21\n",
      "pycryptodome                             3.21.0\n",
      "pydantic                                 2.10.4\n",
      "pydantic_core                            2.27.2\n",
      "pydantic-settings                        2.7.1\n",
      "pydeck                                   0.9.0b0\n",
      "pygame                                   2.1.2\n",
      "Pygments                                 2.17.2\n",
      "PyMuPDF                                  1.24.10\n",
      "PyMuPDFb                                 1.24.10\n",
      "pyodbc                                   5.2.0\n",
      "pyogrio                                  0.10.0\n",
      "pyparsing                                3.0.6\n",
      "pypdf                                    4.3.1\n",
      "PyPDF2                                   3.0.1\n",
      "pypdfium2                                4.30.0\n",
      "PyPika                                   0.48.9\n",
      "pypiwin32                                223\n",
      "pyproj                                   3.7.0\n",
      "pyproject_hooks                          1.1.0\n",
      "pyreadline3                              3.4.1\n",
      "pyrsistent                               0.18.1\n",
      "pyspellchecker                           0.8.1\n",
      "python-dateutil                          2.8.2\n",
      "python-dotenv                            1.0.1\n",
      "python-iso639                            2024.2.7\n",
      "python-magic                             0.4.27\n",
      "python-magic-bin                         0.4.14\n",
      "python-multipart                         0.0.9\n",
      "python-oxmsg                             0.0.1\n",
      "pyttsx3                                  2.90\n",
      "pytz                                     2021.3\n",
      "pywin32                                  303\n",
      "pywinpty                                 1.1.6\n",
      "PyYAML                                   6.0.1\n",
      "pyzmq                                    22.3.0\n",
      "rank-bm25                                0.2.2\n",
      "rapidfuzz                                3.8.1\n",
      "rapidocr-onnxruntime                     1.3.24\n",
      "regex                                    2024.4.16\n",
      "requests                                 2.31.0\n",
      "requests-oauthlib                        1.3.1\n",
      "requests-toolbelt                        1.0.0\n",
      "rich                                     13.7.1\n",
      "rsa                                      4.8\n",
      "s3transfer                               0.10.1\n",
      "safetensors                              0.4.3\n",
      "scikeras                                 0.4.1\n",
      "scikit-learn                             1.0.2\n",
      "scikit-plot                              0.3.7\n",
      "scipy                                    1.14.1\n",
      "seaborn                                  0.13.0\n",
      "Send2Trash                               1.8.0\n",
      "sentence-transformers                    2.7.0\n",
      "setuptools                               58.1.0\n",
      "shapely                                  2.0.6\n",
      "shellingham                              1.5.4\n",
      "six                                      1.16.0\n",
      "smart-open                               7.0.5\n",
      "smmap                                    5.0.1\n",
      "sniffio                                  1.2.0\n",
      "soupsieve                                2.5\n",
      "spacy                                    3.8.2\n",
      "spacy-legacy                             3.0.12\n",
      "spacy-loggers                            1.0.5\n",
      "SpeechRecognition                        3.10.4\n",
      "SQLAlchemy                               2.0.29\n",
      "sqlparse                                 0.5.0\n",
      "srsly                                    2.4.8\n",
      "stack-data                               0.1.4\n",
      "starlette                                0.37.2\n",
      "statsmodels                              0.14.4\n",
      "streamlit                                1.33.0\n",
      "striprtf                                 0.0.26\n",
      "sympy                                    1.12\n",
      "tabulate                                 0.9.0\n",
      "tbb                                      2021.12.0\n",
      "tenacity                                 8.2.3\n",
      "tensorboard                              2.8.0\n",
      "tensorboard-data-server                  0.6.1\n",
      "tensorboard-plugin-wit                   1.8.1\n",
      "tensorflow                               2.8.0\n",
      "tensorflow-io-gcs-filesystem             0.24.0\n",
      "termcolor                                1.1.0\n",
      "terminado                                0.12.1\n",
      "testpath                                 0.5.0\n",
      "textblob                                 0.18.0.post0\n",
      "tf-estimator-nightly                     2.8.0.dev2021122109\n",
      "thinc                                    8.3.2\n",
      "threadpoolctl                            3.1.0\n",
      "tiktoken                                 0.7.0\n",
      "tinycss2                                 1.3.0\n",
      "tokenizers                               0.19.1\n",
      "toml                                     0.10.2\n",
      "tomli                                    1.2.3\n",
      "toolz                                    0.12.1\n",
      "torch                                    2.3.0\n",
      "tornado                                  6.1\n",
      "tqdm                                     4.66.2\n",
      "traitlets                                5.14.3\n",
      "transformers                             4.41.0\n",
      "typer                                    0.12.3\n",
      "types-requests                           2.32.0.20240523\n",
      "typing_extensions                        4.12.2\n",
      "typing-inspect                           0.9.0\n",
      "tzdata                                   2024.1\n",
      "ujson                                    5.10.0\n",
      "unstructured                             0.15.13\n",
      "unstructured-client                      0.22.0\n",
      "urllib3                                  2.2.1\n",
      "utils                                    1.0.2\n",
      "uvicorn                                  0.29.0\n",
      "wasabi                                   1.1.3\n",
      "watchdog                                 4.0.0\n",
      "watchfiles                               0.21.0\n",
      "wcwidth                                  0.2.5\n",
      "weasel                                   0.4.1\n",
      "webencodings                             0.5.1\n",
      "websocket-client                         1.2.3\n",
      "websockets                               12.0\n",
      "Werkzeug                                 2.0.2\n",
      "wheel                                    0.37.1\n",
      "widgetsnbextension                       4.0.11\n",
      "wordcloud                                1.9.3\n",
      "wrapt                                    1.13.3\n",
      "xlrd                                     2.0.1\n",
      "XlsxWriter                               3.2.0\n",
      "xxhash                                   3.4.1\n",
      "yarg                                     0.1.9\n",
      "yarl                                     1.9.4\n",
      "zipp                                     3.18.1\n"
     ]
    }
   ],
   "source": [
    "!py -m pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from uuid import uuid4\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import faiss\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.docstore.document import Document\n",
    "from typing import List, Tuple\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.storage import InMemoryStore\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_DEPLOYMENT_ENDPOINT = \"https://pkl-aa-dev-aiservices.openai.azure.com/\" \n",
    "OPENAI_API_KEY = \"AXEC3y1jC9ZNGCBB12NZwrpBSzScq1esexgvCXiqw7PaHE04vSMbJQQJ99BDACYeBjFXJ3w3AAABACOG4CMN\" \n",
    "OPENAI_API_VERSION = \"2024-12-01-preview\"\n",
    "\n",
    "OPENAI_DEPLOYMENT_NAME = \"gpt-4o\"\n",
    "OPENAI_MODEL_NAME=\"gpt-4o\"\n",
    "embedding_api_version = \"2024-02-01\"\n",
    "\n",
    "# OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "# CHROMA_PATH = \"c:\\\\Users\\\\SujaySunilNagvekar\\\\VM\\\\GEN AI\\\\SERFF\\\\vector_db\\\\testing_db\"\n",
    "DB_PATH  = \"C:\\\\Users\\\\SujaySunilNagvekar\\\\VM\\\\GEN AI\\\\KM\\\\vector_db\"\n",
    "DATA_FOLDER = \"C:\\\\Users\\\\SujaySunilNagvekar\\\\VM\\\\GEN AI\\\\KM\\\\Documents\"\n",
    "\n",
    "### Cohere API key \n",
    "# cohere_API_key = \"wHyiTViP32Y3Q8Qwhjmd4QGNCkYNpxqtsemtSri3\"\n",
    "# co = cohere.Client(cohere_API_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = AzureChatOpenAI(\n",
    "                        temperature=0,\n",
    "                        deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "                        model_name=OPENAI_MODEL_NAME,\n",
    "                        azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "                        openai_api_version=OPENAI_API_VERSION,\n",
    "                        openai_api_key=OPENAI_API_KEY            \n",
    "                    )\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "                        deployment=\"text-embedding-3-small\",\n",
    "                        model=\"text-embedding-3-small\",\n",
    "                        azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "                        openai_api_version=embedding_api_version,\n",
    "                        openai_api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Config\n",
    "\n",
    "# Splitters\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)\n",
    "\n",
    "\n",
    "# Create empty FAISS index\n",
    "dim = len(embeddings.embed_query(\"test\"))  # Get embedding size\n",
    "faiss_index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "\n",
    "# Create embedding-based FAISS vector index for child chunks\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=embeddings,      # your embedding model\n",
    "    index=faiss_index,                  # faiss.IndexFlatL2(dim)\n",
    "    docstore=InMemoryDocstore(),        # stores child chunks\n",
    "    index_to_docstore_id={}             # maps FAISS ids to document IDs\n",
    ")\n",
    "\n",
    "# Create parent document store for full/large chunks\n",
    "docstore = InMemoryStore()\n",
    "    \n",
    "# Parent-Child retriever\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added to parent-child retriever!\n"
     ]
    }
   ],
   "source": [
    "# Load docs\n",
    "docs = []\n",
    "\n",
    "for filename in os.listdir(DATA_FOLDER):\n",
    "    file_path = os.path.join(DATA_FOLDER, filename)\n",
    "\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif filename.endswith(\".docx\"):\n",
    "        loader = UnstructuredWordDocumentLoader(file_path)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    pages = loader.load()\n",
    "    full_text = \"\\n\\n\".join([page.page_content for page in pages])\n",
    "    doc = Document(page_content=full_text, metadata={\"filename\": filename})\n",
    "    docs.append(doc)\n",
    "\n",
    "retriever.add_documents(docs)  # ✅ THIS is critical\n",
    "\n",
    "print(\"✅ Added to parent-child retriever!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(DB_PATH)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"parent_docstore.pkl\", \"wb\") as f:\n",
    "    pickle.dump(retriever.docstore.store, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reload FAISS vectorstore (child chunks)\n",
    "vectorstore = FAISS.load_local(\n",
    "    DB_PATH,\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# 2. Reload parent docstore\n",
    "with open(\"parent_docstore.pkl\", \"rb\") as f:\n",
    "    parent_data = pickle.load(f)\n",
    "\n",
    "docstore = InMemoryStore()\n",
    "docstore.store = parent_data\n",
    "\n",
    "# 3. Rebuild retriever\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Filename: Arch-Lit Guidelines-2014.pdf\n",
      "🧩 Chunk Size: None\n",
      "📏 Length: 1971\n",
      "2                                                           LITIGATION GUIDELINES FOR DEFENSE AND COVERAGE COUNSEL 11/1/14  Team Concept  \n",
      "The team concept is best implemented if the cases assigned b y Arch Insur ance are con sistently \n",
      "handled by a core group of legal professionals in your office.  \n",
      "---\n",
      "\n",
      "📄 Filename: Leader SW_Final_1.30.18.docx\n",
      "🧩 Chunk Size: None\n",
      "📏 Length: 1991\n",
      "Auto Investigation Supervisor Frequency Task Key Outputs Weekly Workload Management Utilize reporting to evaluate employee productivity and compliance with established workload expectations (based on KPIs) Daily Case Reviews Review and provide feedback on all files at specific points in the claim li \n",
      "---\n",
      "\n",
      "📄 Filename: Leader SW_Final_1.30.18.docx\n",
      "🧩 Chunk Size: None\n",
      "📏 Length: 920\n",
      "summary of prior reporting period KPIs and results Share performance results with applicable parties Share individual performance during 1:1s Monthly 1:1 with Direct Reports Discuss all applicable performance and productivity metrics Performance management discussions, as warranted Recognition Mento \n",
      "---\n",
      "\n",
      "📄 Filename: Leader SW_Final_1.30.18.docx\n",
      "🧩 Chunk Size: None\n",
      "📏 Length: 654\n",
      "as warranted Recognition Mentoring and coaching opportunities identified Individual employee development plans Monthly 2-3 employees per month Desk Rides, Shadowing and/or Ride Along Observe day-to-day activities Technical competence Efficiency Customer interactions Provide and document feedback and \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = retriever.invoke(\"Team Concept\")\n",
    "for doc in results:\n",
    "    print(\"📄 Filename:\", doc.metadata.get(\"filename\"))\n",
    "    print(\"🧩 Chunk Size:\", doc.metadata.get(\"chunk_size\"))\n",
    "    print(\"📏 Length:\", len(doc.page_content))\n",
    "    print(doc.page_content[:300], \"\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert in interpreting SERFF (System for Electronic Rates & Forms Filing) documents. \n",
    "Your task is to provide clear, concise, and accurate answers based on the retrieved document context. \n",
    "The context includes both **parent** and **child** chunks of the relevant documents.\n",
    "\n",
    "### **Instructions:**\n",
    "- Provide answers based strictly on the provided context.\n",
    "- Do not include irrelevant information.\n",
    "- Avoid any form of hallucination or speculation.\n",
    "- Present answers in bullet points for clarity and conciseness.\n",
    "- If the answer is contained in a table, preserve the table structure and format.\n",
    "- The Answer should be detailed and expansive\n",
    "- For questions needing specific answers from a table, take as much time as needed to answer accurately. Pay special attention to numbers and counties in the question\n",
    "\n",
    "---\n",
    "\n",
    "### **User Query:**\n",
    "{query}\n",
    "\n",
    "---\n",
    "\n",
    "### **Document Context:**\n",
    "\n",
    "**Parent Chunks (Broader Context):**\n",
    "{parent_context}\n",
    "\n",
    "**Child Chunks (Specific Details):**\n",
    "{child_context}\n",
    "\n",
    "**Historical Context (Chat History Summary):**\n",
    "{history_summary}\n",
    "---\n",
    "\n",
    "### **Your Answer:**\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the prompt with ChatPromptTemplate\n",
    "serff_prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the last 5 question-answer pairs\n",
    "conversation_history = []\n",
    "history_limit = 5  # Keep up to the last 5 exchanges\n",
    "\n",
    "def update_conversation_history(question, answer):\n",
    "    \"\"\"\n",
    "    Store the last 5 interactions (Standalone Query + LLM Response).\n",
    "    \"\"\"\n",
    "    conversation_history.append(f\"Question: {question}\\nAnswer: {answer}\")\n",
    "    if len(conversation_history) > history_limit:\n",
    "        conversation_history.pop(0)  # Keep only the latest 5 entries\n",
    "\n",
    "def clear_history():\n",
    "    \"\"\"\n",
    "    Clear the entire conversation history.\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    print(\"🧹 Conversation history cleared!\")\n",
    "\n",
    "def get_conversation_context():\n",
    "    \"\"\"\n",
    "    Return the concatenated conversation history.\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(conversation_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_answer_the_query(query, llm, retriever, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve relevant chunks using hybrid retrieval. Reformulate and summarize only if chat history exists.\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Searching for: {query}\")\n",
    "\n",
    "    # Step 1: Check if there's existing conversation history\n",
    "    conversation_context = get_conversation_context()\n",
    "\n",
    "    if conversation_context:\n",
    "        # ✅ Summarize the previous Q&A to add as context\n",
    "        summary_prompt = f\"\"\"\n",
    "        You are an expert in understanding SERFF document conversations.\n",
    "        Given the following conversation history, provide a concise summary that captures key points. Extract Topic , Entities , Theme ( one liner ) if any, to the best of your ability.\n",
    "\n",
    "        Conversation History:\n",
    "        {conversation_context}\n",
    "\n",
    "        Enities:\n",
    "        Theme:\n",
    "        Summary:\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        summary_response = llm.invoke(summary_prompt, max_tokens=MAX_HISTORY_SUMMARY_TOKENS)\n",
    "        summary_duration = time.time() - start_time        \n",
    "        history_summary = summary_response.content.strip()\n",
    "        summary_tokens = count_tokens(summary_prompt) + count_tokens(summary_response.content)\n",
    "        print(f\"📝 History Summary Generated in {summary_duration:.2f} seconds | Tokens Used: {summary_tokens}\")\n",
    "\n",
    "        # ✅ Reformulate the new query based on chat history\n",
    "        reformulation_prompt = f\"\"\"\n",
    "        Given the conversation history and the user's latest question, reformulate the user query as a standalone query.\n",
    "\n",
    "        Conversation History:\n",
    "        {conversation_context}\n",
    "\n",
    "        Latest Question:\n",
    "        {query}\n",
    "\n",
    "        Reformulated Question:\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        reformulated_response = llm.invoke(reformulation_prompt)\n",
    "        reformulation_duration = time.time() - start_time\n",
    "        standalone_query = reformulated_response.content.strip()\n",
    "        reformulation_tokens = count_tokens(reformulation_prompt) + count_tokens(reformulated_response.content)\n",
    "        print(f\"🔄 Reformulated Query: {standalone_query} | Time: {reformulation_duration:.2f}s | Tokens: {reformulation_tokens}\")\n",
    "    else:\n",
    "        # No history: Use the original query directly\n",
    "        standalone_query = query\n",
    "        history_summary = \"\"  # No summary for the first question\n",
    "\n",
    "    # Step 2: Retrieve Top-K Child Chunks using Hybrid Retrieval\n",
    "    print(standalone_query)\n",
    "    start_time = time.time()\n",
    "    retrieved_children = retriever.invoke(standalone_query)\n",
    "    retrieval_duration = time.time() - start_time\n",
    "    print(f\"✅ Retrieved {len(retrieved_children)} Child Chunks in {retrieval_duration:.2f} seconds\")\n",
    "    \n",
    "    # Extract unique parent IDs from retrieved child chunks\n",
    "    parent_ids = list(set([doc.metadata[\"related_to\"] for doc in retrieved_children]))    \n",
    "    print(f\"📥 Fetching Parent Chunks for Parent_IDs: {parent_ids}\")\n",
    "\n",
    "    # Step 3: Retrieve Corresponding Parent Chunks based on Parent IDs\n",
    "    start_time = time.time()\n",
    "    parent_results = parent_db.similarity_search(standalone_query, k=5, filter={\"ID\": {\"$in\": parent_ids}})\n",
    "    parent_retrieval_duration = time.time() - start_time\n",
    "    print(f\"✅ Retrieved {len(parent_results)} Parent Chunks in {parent_retrieval_duration:.2f} seconds\")\n",
    "\n",
    "    # Step 4: Prepare contexts\n",
    "    parent_context = \"\\n\\n\".join([doc.page_content for doc in parent_results])\n",
    "    child_context = \"\\n\\n\".join([doc.page_content for doc in retrieved_children])\n",
    "\n",
    "    # Step 5: Format the final prompt with query, retrieved context, and history summary\n",
    "    formatted_prompt = serff_prompt_template.format(\n",
    "        query=standalone_query,\n",
    "        parent_context=parent_context,\n",
    "        child_context=child_context,\n",
    "        history_summary=history_summary\n",
    "    )\n",
    "\n",
    "    # Step 6: Generate final response from LLM\n",
    "    start_time = time.time()\n",
    "    response = llm.invoke(formatted_prompt, max_tokens=MAX_RESPONSE_TOKENS)\n",
    "    response_duration = time.time() - start_time\n",
    "    response_tokens = count_tokens(formatted_prompt) + count_tokens(response.content)\n",
    "    answer = response.content.strip()\n",
    "    print(f\"💬 LLM Response Generated in {response_duration:.2f} seconds | Tokens Used: {response_tokens}\")\n",
    "    print(f\"💬 Response: {answer}\")\n",
    "\n",
    "    # Step 7: Update conversation history with the reformulated query and response\n",
    "    update_conversation_history(standalone_query, answer)\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Conversation history cleared!\n"
     ]
    }
   ],
   "source": [
    "get_conversation_context()\n",
    "clear_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_state_change(selected_state):\n",
    "    global hybrid_retriever\n",
    "    hybrid_retriever = initialize_retriever(selected_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 No filter applied. Retrieving from all documents.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Step 1: User selects a state\n",
    "selected_state = None # Example from UI dropdown\n",
    "hybrid_retriever = initialize_retriever(selected_state=selected_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching for: What is Binding Authority for Crop Insurance Underwriting for Indiana\n",
      "What is Binding Authority for Crop Insurance Underwriting for Indiana\n",
      "✅ Retrieved 4 Child Chunks in 0.88 seconds\n",
      "📥 Fetching Parent Chunks for Parent_IDs: ['indiana:7:0', 'indiana:1:0', 'indiana:2:0', 'general:15:2']\n",
      "✅ Retrieved 4 Parent Chunks in 0.41 seconds\n",
      "💬 LLM Response Generated in 2.09 seconds | Tokens Used: 2370\n",
      "💬 Response: Based on the provided context, the answer to the user query \"What is Binding Authority for Crop Insurance Underwriting for Indiana?\" is:\n",
      "\n",
      "- Coverage for AABIC private insurance policies is bound by submitting a properly completed, signed, and dated application.\n",
      "- An application is submitted when it has been properly keyed and scanned into the processing system.\n",
      "- Coverage can be bound in two ways:\n",
      "  1. Policy provisions shall take effect at 12:01 a.m. the following day after the application is signed and dated by the policyholder and agent provided that the complete application is keyed into the processing system and placed in the “Ready to Bind” or “Submit” status within 72 hours of the signature.\n",
      "  2. Policy provisions for electronic or verbal request for coverage shall take effect at 12:01 a.m. the following day after the request for coverage is received via acceptable delivery (Email, Fax, Phone) by the Regional Hail Underwriter, provided the request includes the name of the insured, state and county or parish, the desired plan(s).\n",
      "- There is no binding authority for specialty crops without prior written approval from AABIC Private Products department.\n"
     ]
    }
   ],
   "source": [
    "# First query\n",
    "query1 = \"What is Binding Authority for Crop Insurance Underwriting for Indiana\"\n",
    "response1 = retrieve_and_answer_the_query(query1, llm, hybrid_retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching for: List Form Code and IPA factor for South Dakota\n",
      "📝 History Summary Generated in 0.73 seconds | Tokens Used: 207\n",
      "🔄 Reformulated Query: What are the Form Code and IPA factor for South Dakota? | Time: 0.21s | Tokens: 145\n",
      "What are the Form Code and IPA factor for South Dakota?\n",
      "✅ Retrieved 4 Child Chunks in 0.27 seconds\n",
      "📥 Fetching Parent Chunks for Parent_IDs: ['south_dakota:2:6', 'south_dakota:1:6', 'general:20:1']\n",
      "✅ Retrieved 3 Parent Chunks in 0.08 seconds\n",
      "💬 LLM Response Generated in 1.68 seconds | Tokens Used: 2292\n",
      "💬 Response: Based on the provided context, the Form Code and IPA factor for South Dakota are as follows:\n",
      "\n",
      "- Form Code: Companion 2, Companion 3, Companion 70\n",
      "- IPA Factor:\n",
      "  - Companion 2: 1.50\n",
      "  - Companion 3: 2.00\n",
      "  - Companion 70: 2.66\n",
      "\n",
      "It is important to note that these are the maximum IPA factors for insureds with 100% share and listed maximum IPA of $1,000. Any requests for coverage beyond the stated limits must receive approval by AABIC, and approval requests must be accompanied by a demonstration of the producer to produce the value of the requested coverage level.\n",
      "\n",
      "Additionally, the provided context includes information on crop insurance liability limits in South Dakota, standard crops, and catastrophe loss awards. However, it does not contain specific liability limits for each crop in South Dakota underwriting rules. It is recommended to consult with a crop insurance agent or the AABIC Service Office for more specific information.\n"
     ]
    }
   ],
   "source": [
    "# First query\n",
    "query2 = \"List Form Code and IPA factor for South Dakota\"\n",
    "response2  = retrieve_and_answer_the_query(query2, llm, hybrid_retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching for: what is the Catastrophe Loss Award Policy\n",
      "📝 History Summary Generated in 0.96 seconds | Tokens Used: 461\n",
      "🔄 Reformulated Query: Can you provide information on the Catastrophe Loss Award Policy? | Time: 0.23s | Tokens: 367\n",
      "Can you provide information on the Catastrophe Loss Award Policy?\n",
      "✅ Retrieved 5 Child Chunks in 0.33 seconds\n",
      "📥 Fetching Parent Chunks for Parent_IDs: ['colorado:2:8', 'colorado:1:6', 'iowa:1:8', 'general:3:0', 'general:20:2']\n",
      "✅ Retrieved 5 Parent Chunks in 0.14 seconds\n",
      "💬 LLM Response Generated in 2.04 seconds | Tokens Used: 2863\n",
      "💬 Response: Based on the provided document context, the following information can be provided regarding the Catastrophe Loss Award Policy:\n",
      "\n",
      "- The Catastrophe Loss Award applies to all policies below that pay 70% indemnity at 70% loss.\n",
      "- Once the ascertained percentage of loss per acre exceeds 70%, an additional one-half of the percent of loss that is in excess of 70% will be paid. However, the total percentage payable shall not exceed 100%.\n",
      "- The Catastrophe Loss Award is described in the Crop Insurance rules for Iowa and Colorado.\n",
      "- The document context does not provide information on the Catastrophe Loss Award Policy for other states.\n",
      "- The provided document context does not contain specific liability limits for each crop in South Dakota underwriting rules.\n",
      "- The Form Code and IPA factor for Colorado and Iowa are provided, with maximum limits for insureds.\n",
      "- Any requests for coverage beyond the stated limits must receive approval by AABIC. Approval requests must be accompanied by a demonstration of the producer to produce the value of the requested coverage level.\n",
      "- It is recommended to consult with a crop insurance agent or the AABIC Service Office for more specific information on the Catastrophe Loss Award Policy.\n"
     ]
    }
   ],
   "source": [
    "# First query\n",
    "query3 = \"what is the Catastrophe Loss Award Policy\"\n",
    "response3  = retrieve_and_answer_the_query(query3, llm, hybrid_retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching for: Describe DDB, DDC, DDA definition for South Dakota \n",
      "📝 History Summary Generated in 1.10 seconds | Tokens Used: 737\n",
      "🔄 Reformulated Query: What are the definitions of DDB, DDC, and DDA in South Dakota? | Time: 0.25s | Tokens: 639\n",
      "What are the definitions of DDB, DDC, and DDA in South Dakota?\n",
      "✅ Retrieved 5 Child Chunks in 0.28 seconds\n",
      "📥 Fetching Parent Chunks for Parent_IDs: ['south_dakota:2:0', 'south_dakota:2:10', 'south_dakota:1:19', 'south_dakota:2:23', 'south_dakota:1:21']\n",
      "✅ Retrieved 5 Parent Chunks in 0.14 seconds\n",
      "💬 LLM Response Generated in 2.67 seconds | Tokens Used: 4290\n",
      "💬 Response: Based on the provided document context, the definitions of DDB, DDC, and DDA in South Dakota are as follows:\n",
      "\n",
      "- DDA: Provides a loss payment when the adjusted loss per acre exceeds 10%. The percentage per acre then payable will be the percent in excess of 10%. When the percent of loss exceeds 20%, an additional 2% will be paid for each percent in excess of 20%. Once the adjusted loss exceeds 25%, this provision no longer applies.\n",
      "- DDB: Provides a loss payment when the adjusted loss per acre exceeds 20%. The percentage per acre then payable will be the percent in excess of 20%. When the percent of loss exceeds 30%, an additional 2% will be paid for each percent in excess of 30%. Once the adjusted loss exceeds 40%, this provision no longer applies.\n",
      "- DDC: Provides a loss payment when the adjusted loss per acre exceeds 30%. The percentage per acre then payable will be the percent in excess of 30%. Once the percent of loss exceeds 40%, an additional 2% will be paid for each percent of loss in excess of 40%. Once the adjusted loss per acre exceeds 55%, this provision no longer applies. \n",
      "\n",
      "Note: The document context does not provide specific liability limits for each crop in South Dakota underwriting rules. It is recommended to consult with a crop insurance agent or the AABIC Service Office for more specific information.\n"
     ]
    }
   ],
   "source": [
    "# First query\n",
    "query4 = \"Describe DDB, DDC, DDA definition for South Dakota \"\n",
    "response4  = retrieve_and_answer_the_query(query4, llm, hybrid_retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>metadatas</th>\n",
       "      <th>documents</th>\n",
       "      <th>uris</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00158af4-3417-4a40-bcd7-eccf842eb5b1</td>\n",
       "      <td>None</td>\n",
       "      <td>{'ID': 'south_dakota:1:14:5', 'LOB': 'American Agri-Business Insurance Company', 'Parent_ID': 'south_dakota:14', 'State': 'south_dakota', 'chunk_size': 400, 'filename': 'ARMT-132998838-2022 SD Crop Hail Rules 12.08.21.pdf', 'related_to': 'south_dakota:1:14'}</td>\n",
       "      <td>2020 -NCIS 562   \\nThis mandatory endorsement amends your policy for coverage on hemp grown for \\nharvest as fiber.  XS10 deductible.  Premium for this endorsement is due at the time of \\napplication.  \\n  \\nSOUTH DAKOTA  OPTIONAL ENDORSEMENTS \\nIntroduction  Below are general descriptions of the optional endorsements available with a Crop Hail</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0023319d-1207-4ee9-adce-ff0acc984cb4</td>\n",
       "      <td>None</td>\n",
       "      <td>{'ID': 'indiana:3:14:5', 'LOB': 'American Agri-Business Insurance Company', 'Parent_ID': 'indiana:14', 'State': 'indiana', 'chunk_size': 400, 'filename': 'ARMT-132998817-2022 IN Rates (Formatted) 08.24.21.pdf', 'related_to': 'indiana:3:14'}</td>\n",
       "      <td>105 Monroe CP2 0.50 1.10 0.70 7.45 1.25 0.85\\n105 Monroe CP3 0.70 1.50 1.00 9.95 1.75 1.20\\n105 Monroe CP4 0.85 1.85 1.20 11.50 2.15 1.50\\n105 Monroe XS5IP 4.10\\n105 Monroe XS10IP 3.20\\n105 Monroe XS15 9.25\\n105 Monroe XS15IP 1.60 3.10 3.35\\n105 Monroe XS20 0.80\\n105 Monroe XS10 0.80\\n107 Montgomery BASIC 0.45 1.00 0.65 2.15 4.15 4.50 1.15 0.80\\n107 Montgomery DXS5 0.30 0.75 0.45 0.75 0.55</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>ffd8de8b-b37f-44f3-9705-dd45fd6c1964</td>\n",
       "      <td>None</td>\n",
       "      <td>{'ID': 'indiana:6:0:2', 'LOB': 'American Agri-Business Insurance Company', 'Parent_ID': 'indiana:0', 'State': 'indiana', 'chunk_size': 400, 'filename': 'ARMT-132998817-IN Objection Responses 01.26.22.pdf', 'related_to': 'indiana:6:0'}</td>\n",
       "      <td>Objection 2: \\nCompany experience and/or competitive comparisons that led to the Wind Plus/Extra Harvest Expense \\nwind rate adjustments, along with the actual adjustments.  \\n \\nResponse 2: \\nSee the graphs below  and on the next page for  competitive comparisons  and company adjustments .</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>ffe171bd-172e-4bc8-99eb-1248c1031c95</td>\n",
       "      <td>None</td>\n",
       "      <td>{'ID': 'iowa:4:184:1', 'LOB': 'American Agri-Business Insurance Company', 'Parent_ID': 'iowa:184', 'State': 'iowa', 'chunk_size': 400, 'filename': 'ARMT-132998818-2022 IA Production Wind Rates (Formatted) 12.03.21.pdf', 'related_to': 'iowa:4:184'}</td>\n",
       "      <td>191 Winneshiek 105/50 3.10 3.00 3.65 191 Winneshiek 105/50 3.55 3.40 4.15\\n191 Winneshiek 105/55 3.40 3.25 3.95 191 Winneshiek 105/55 3.85 3.70 4.50\\n191 Winneshiek 105/60 3.70 3.55 4.35 191 Winneshiek 105/60 4.25 4.05 4.95\\n191 Winneshiek 105/65 4.10 3.95 4.80 191 Winneshiek 105/65 4.70 4.50 5.45\\n191 Winneshiek 105/70 4.55 4.40 5.30 191 Winneshiek 105/70 5.20 5.00 6.05</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3781 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ids embeddings  \\\n",
       "0     00158af4-3417-4a40-bcd7-eccf842eb5b1       None   \n",
       "1     0023319d-1207-4ee9-adce-ff0acc984cb4       None   \n",
       "...                                    ...        ...   \n",
       "3779  ffd8de8b-b37f-44f3-9705-dd45fd6c1964       None   \n",
       "3780  ffe171bd-172e-4bc8-99eb-1248c1031c95       None   \n",
       "\n",
       "                                                                                                                                                                                                                                                               metadatas  \\\n",
       "0     {'ID': 'south_dakota:1:14:5', 'LOB': 'American Agri-Business Insurance Company', 'Parent_ID': 'south_dakota:14', 'State': 'south_dakota', 'chunk_size': 400, 'filename': 'ARMT-132998838-2022 SD Crop Hail Rules 12.08.21.pdf', 'related_to': 'south_dakota:1:14'}   \n",
       "1                       {'ID': 'indiana:3:14:5', 'LOB': 'American Agri-Business Insurance Company', 'Parent_ID': 'indiana:14', 'State': 'indiana', 'chunk_size': 400, 'filename': 'ARMT-132998817-2022 IN Rates (Formatted) 08.24.21.pdf', 'related_to': 'indiana:3:14'}   \n",
       "...                                                                                                                                                                                                                                                                  ...   \n",
       "3779                          {'ID': 'indiana:6:0:2', 'LOB': 'American Agri-Business Insurance Company', 'Parent_ID': 'indiana:0', 'State': 'indiana', 'chunk_size': 400, 'filename': 'ARMT-132998817-IN Objection Responses 01.26.22.pdf', 'related_to': 'indiana:6:0'}   \n",
       "3780             {'ID': 'iowa:4:184:1', 'LOB': 'American Agri-Business Insurance Company', 'Parent_ID': 'iowa:184', 'State': 'iowa', 'chunk_size': 400, 'filename': 'ARMT-132998818-2022 IA Production Wind Rates (Formatted) 12.03.21.pdf', 'related_to': 'iowa:4:184'}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                     documents  \\\n",
       "0                                                   2020 -NCIS 562   \\nThis mandatory endorsement amends your policy for coverage on hemp grown for \\nharvest as fiber.  XS10 deductible.  Premium for this endorsement is due at the time of \\napplication.  \\n  \\nSOUTH DAKOTA  OPTIONAL ENDORSEMENTS \\nIntroduction  Below are general descriptions of the optional endorsements available with a Crop Hail   \n",
       "1     105 Monroe CP2 0.50 1.10 0.70 7.45 1.25 0.85\\n105 Monroe CP3 0.70 1.50 1.00 9.95 1.75 1.20\\n105 Monroe CP4 0.85 1.85 1.20 11.50 2.15 1.50\\n105 Monroe XS5IP 4.10\\n105 Monroe XS10IP 3.20\\n105 Monroe XS15 9.25\\n105 Monroe XS15IP 1.60 3.10 3.35\\n105 Monroe XS20 0.80\\n105 Monroe XS10 0.80\\n107 Montgomery BASIC 0.45 1.00 0.65 2.15 4.15 4.50 1.15 0.80\\n107 Montgomery DXS5 0.30 0.75 0.45 0.75 0.55   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                        ...   \n",
       "3779                                                                                                       Objection 2: \\nCompany experience and/or competitive comparisons that led to the Wind Plus/Extra Harvest Expense \\nwind rate adjustments, along with the actual adjustments.  \\n \\nResponse 2: \\nSee the graphs below  and on the next page for  competitive comparisons  and company adjustments .   \n",
       "3780                     191 Winneshiek 105/50 3.10 3.00 3.65 191 Winneshiek 105/50 3.55 3.40 4.15\\n191 Winneshiek 105/55 3.40 3.25 3.95 191 Winneshiek 105/55 3.85 3.70 4.50\\n191 Winneshiek 105/60 3.70 3.55 4.35 191 Winneshiek 105/60 4.25 4.05 4.95\\n191 Winneshiek 105/65 4.10 3.95 4.80 191 Winneshiek 105/65 4.70 4.50 5.45\\n191 Winneshiek 105/70 4.55 4.40 5.30 191 Winneshiek 105/70 5.20 5.00 6.05   \n",
       "\n",
       "      uris  data  \n",
       "0     None  None  \n",
       "1     None  None  \n",
       "...    ...   ...  \n",
       "3779  None  None  \n",
       "3780  None  None  \n",
       "\n",
       "[3781 rows x 6 columns]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set Pandas options to display all rows and columns\n",
    "pd.set_option('display.max_rows',5)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content of each column\n",
    "\n",
    "# Now display the DataFrame\n",
    "pd.DataFrame(child_db.get(include=[\"metadatas\", \"documents\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    def reformulate_question(self, query_text, llm):\n",
    "        \"\"\"\n",
    "        Reformulate the follow-up question using LLM based on the conversation history.\n",
    "        \"\"\"\n",
    "        # Get the conversation history\n",
    "        conversation_context = self.get_conversation_context()\n",
    "\n",
    "        # Use the reformulation prompt to generate a standalone question\n",
    "        reformulation_prompt = \"\"\"\n",
    "        Given a chat history and the latest user question which might reference context in the chat history, \n",
    "        formulate a standalone question which can be understood without the chat history. Do NOT answer the question, \n",
    "        just reformulate it if needed and otherwise return it as is.\n",
    "\n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "\n",
    "        Latest Question:\n",
    "        {latest_question}\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare the input for the LLM model\n",
    "        formatted_prompt = reformulation_prompt.format(chat_history=conversation_context, latest_question=query_text)\n",
    "\n",
    "        # Get the reformulated question from LLM\n",
    "        reformulated_response = llm.invoke(input=formatted_prompt)\n",
    "        standalone_question = reformulated_response.content.strip()\n",
    "\n",
    "        return standalone_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making a chain of conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# Assuming `retriever` is your document retrieval mechanism, already defined\n",
    "db = Chroma(\n",
    "        persist_directory=CHROMA_PATH, embedding_function=get_embedding()\n",
    "    )\n",
    "retriever = db.as_retriever(search_type=\"mmr\",\n",
    "    search_kwargs={'k': 20,'lambda_mult': 0.05}\n",
    ")  # Define your document retriever here\n",
    "\n",
    "# Contextualization prompt template\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the history-aware retriever chain\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
