{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph-checkpoint-sqlite\n",
      "  Downloading langgraph_checkpoint_sqlite-2.0.10-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting aiosqlite>=0.20 (from langgraph-checkpoint-sqlite)\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting langgraph-checkpoint>=2.0.21 (from langgraph-checkpoint-sqlite)\n",
      "  Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting sqlite-vec>=0.1.6 (from langgraph-checkpoint-sqlite)\n",
      "  Downloading sqlite_vec-0.1.6-py3-none-win_amd64.whl.metadata (198 bytes)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiosqlite>=0.20->langgraph-checkpoint-sqlite) (4.12.2)\n",
      "Requirement already satisfied: langchain-core>=0.2.38 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.3.29)\n",
      "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite)\n",
      "  Downloading ormsgpack-1.10.0-cp310-cp310-win_amd64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.1.147)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (23.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.10.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (8.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.4)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.27.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sujaysunilnagvekar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.2.1)\n",
      "Downloading langgraph_checkpoint_sqlite-2.0.10-py3-none-any.whl (30 kB)\n",
      "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl (44 kB)\n",
      "Downloading sqlite_vec-0.1.6-py3-none-win_amd64.whl (281 kB)\n",
      "Downloading ormsgpack-1.10.0-cp310-cp310-win_amd64.whl (121 kB)\n",
      "Installing collected packages: sqlite-vec, ormsgpack, aiosqlite, langgraph-checkpoint, langgraph-checkpoint-sqlite\n",
      "  Attempting uninstall: langgraph-checkpoint\n",
      "    Found existing installation: langgraph-checkpoint 2.0.9\n",
      "    Uninstalling langgraph-checkpoint-2.0.9:\n",
      "      Successfully uninstalled langgraph-checkpoint-2.0.9\n",
      "Successfully installed aiosqlite-0.21.0 langgraph-checkpoint-2.0.26 langgraph-checkpoint-sqlite-2.0.10 ormsgpack-1.10.0 sqlite-vec-0.1.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!py -m pip install langgraph-checkpoint-sqlite\n",
    "\n",
    "# !py -m pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai langchain-chroma bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from uuid import uuid4\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.docstore.document import Document\n",
    "from typing import List, Tuple\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import re\n",
    "from typing import Annotated, TypedDict\n",
    "import operator\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_DEPLOYMENT_ENDPOINT = \"https://advancedanalyticsopenaikey.openai.azure.com/\"\n",
    "OPENAI_DEPLOYMENT_ENDPOINT_embed = \"https://pkl-aa-dev-aiservices.openai.azure.com/\" \n",
    "OPENAI_API_KEY = \"FqFd4DBx1W97MSVjcZvdQsmQlhI80hXjl48iWYmZ4W3NutUlWvf0JQQJ99BDACYeBjFXJ3w3AAABACOGl3xo\" \n",
    "OPENAI_API_VERSION = \"2024-12-01-preview\"\n",
    "OPENAI_API_KEY_EMBEDDINGS = \"AXEC3y1jC9ZNGCBB12NZwrpBSzScq1esexgvCXiqw7PaHE04vSMbJQQJ99BDACYeBjFXJ3w3AAABACOG4CMN\" \n",
    "OPENAI_DEPLOYMENT_NAME = \"gpt-4o\"\n",
    "OPENAI_MODEL_NAME=\"gpt-4o\"\n",
    "embedding_api_version = \"2024-02-01\"\n",
    "\n",
    "# OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "# CHROMA_PATH = \"c:\\\\Users\\\\SujaySunilNagvekar\\\\VM\\\\GEN AI\\\\SERFF\\\\vector_db\\\\testing_db\"\n",
    "DB_PATH  = \"C:\\\\Users\\\\SujaySunilNagvekar\\\\VM\\\\GEN AI\\\\KM\\\\vector_db\"\n",
    "DATA_FOLDER = \"C:\\\\Users\\\\SujaySunilNagvekar\\\\VM\\\\GEN AI\\\\KM\\\\Documents\"\n",
    "\n",
    "### Cohere API key \n",
    "# cohere_API_key = \"wHyiTViP32Y3Q8Qwhjmd4QGNCkYNpxqtsemtSri3\"\n",
    "# co = cohere.Client(cohere_API_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = AzureChatOpenAI(\n",
    "                        temperature=0.7,\n",
    "                        deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "                        model_name=OPENAI_MODEL_NAME,\n",
    "                        azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "                        openai_api_version=OPENAI_API_VERSION,\n",
    "                        openai_api_key=OPENAI_API_KEY            \n",
    "                    )\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "                        deployment=\"text-embedding-3-small\",\n",
    "                        model=\"text-embedding-3-small\",\n",
    "                        azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT_embed,\n",
    "                        openai_api_version=embedding_api_version,\n",
    "                        openai_api_key=OPENAI_API_KEY_EMBEDDINGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating FNOL and adjuster notes...\n",
      "\n",
      "\n",
      "‚úÖ Done in 7475.98 seconds. File saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "import random\n",
    "from time import time\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize(\"NFKC\", text).encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "\n",
    "def fix_smart_quotes(text):\n",
    "    return (\n",
    "        text.replace(\"‚Äò\", \"'\").replace(\"‚Äô\", \"'\")\n",
    "            .replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"')\n",
    "            .replace(\"‚Äì\", \"-\").replace(\"‚Äî\", \"-\")\n",
    "    )\n",
    "\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"C://Users//SujaySunilNagvekar//VM//GEN AI//KM//new_claims_data.csv\", encoding=\"cp1252\")\n",
    "# Load policy data\n",
    "policy_df = pd.read_csv(\"C://Users//SujaySunilNagvekar//VM//GEN AI//KM//new_policy_data.csv\", encoding=\"cp1252\")\n",
    "\n",
    "\n",
    "class ClaimNotes(BaseModel):\n",
    "    fnol_call: str = Field(description=\"Transcript of the FNOL call between customer and agent.\")\n",
    "    adjuster_notes: str = Field(description=\"Adjuster's notes in bullet or paragraph format.\")\n",
    "\n",
    "# ------------------------------\n",
    "# Prompt Template\n",
    "# ------------------------------\n",
    "# Prompts options for variation\n",
    "tones = [\n",
    "    \"Be warm and empathetic, make the agent sound reassuring.\",\n",
    "    \"Be brief and to the point, like a rushed weekday call.\",\n",
    "    \"Include clarifying questions by the agent.\",\n",
    "    \"Make the customer slightly emotional but cooperative.\",\n",
    "    \"Add a follow-up suggestion by the agent.\",\n",
    "    \"Agent is confident and efficient, but polite.\",\n",
    "    \"Agent uses layman‚Äôs language to explain coverage.\",\n",
    "    \"Agent deals with a mildly frustrated customer.\",\n",
    "    \"Agent reassures about payment processing.\",\n",
    "    \"Include a moment where the customer expresses gratitude.\"\n",
    "]\n",
    "\n",
    "note_styles = [\n",
    "    \"Use bullet points for adjuster notes.\",\n",
    "    \"Use a narrative paragraph style.\",\n",
    "    \"Include a timestamp and adjuster name.\",\n",
    "    \"Add follow-up actions or pending steps in the notes.\",\n",
    "    \"Summarize with a conclusion at the end.\"\n",
    "]\n",
    "\n",
    "note_scenarios = [\n",
    "    \"This is an initial FNOL entry. Just summarize what was reported and next steps.\",\n",
    "    \"The claim is under review. Include pending documentation, follow-ups, and internal status.\",\n",
    "    \"This is a follow-up after initial processing. Include updated estimate or new details.\",\n",
    "    \"The claim has been resolved. Summarize the decision and any payment issued.\",\n",
    "    \"There is a delay or irregularity in the claim. Highlight concerns or next steps.\",\n",
    "    \"Claim was reopened due to new evidence or customer request.\",\n",
    "    \"Customer was contacted multiple times, summarize call attempts or missed connections.\",\n",
    "    \"Adjuster flagged the need for internal legal or fraud team to assess the situation.\",\n",
    "    \"Include interaction with a 3rd party (e.g., Safelite, towing company, repair contractor).\"\n",
    "]\n",
    "\n",
    "\n",
    "agent_profiles = [\n",
    "    \"Agent is a new trainee reading from a script.\",\n",
    "    \"Agent has 20+ years of experience and uses confident, professional language.\",\n",
    "    \"Agent is friendly and conversational, tries to make the customer feel at ease.\",\n",
    "    \"Agent speaks with a Midwest/U.S. regional tone.\",\n",
    "    \"Agent works in the commercial claims department and is used to businesses, not individuals.\",\n",
    "    \"Agent is clearly working remotely and occasionally pauses to look things up.\"\n",
    "]\n",
    "\n",
    "claim_moods = [\n",
    "    \"Customer is very frustrated due to a long wait.\",\n",
    "    \"Customer is surprised by how quickly they were contacted.\",\n",
    "    \"Customer is sad because this claim is related to a recent personal loss.\",\n",
    "    \"Customer is confused about what is covered.\",\n",
    "    \"Customer is trying to remain positive despite the situation.\",\n",
    "    \"Customer is defensive and worried they may be blamed.\"\n",
    "]\n",
    "\n",
    "claim_complexities = [\n",
    "    \"The incident occurred while the customer was traveling out-of-state.\",\n",
    "    \"There are multiple parties involved and conflicting accounts.\",\n",
    "    \"Customer had just renewed their policy days before the loss.\",\n",
    "    \"Photos provided are blurry and hard to verify damage.\",\n",
    "    \"Claim involves prior similar damage that may not be covered.\",\n",
    "    \"Customer submitted the claim late and is unsure if it will be accepted.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Generate outputs\n",
    "fnol_calls, adjuster_notes = [], []\n",
    "start_time = time()\n",
    "print(\"üîÑ Generating FNOL and adjuster notes...\\n\")\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        # Get claim info\n",
    "        claim_data = row.to_dict()\n",
    "        row_info = \"\\n\".join([f\"{k}: {v}\" for k, v in claim_data.items()])\n",
    "\n",
    "        # Match policy by 'Policy Number'\n",
    "        policy_number = claim_data.get(\"Policy Number\")\n",
    "        policy_row = policy_df[policy_df[\"Policy Number\"] == policy_number]\n",
    "        if not policy_row.empty:\n",
    "            policy_info = \"\\n\".join([f\"{k}: {v}\" for k, v in policy_row.iloc[0].to_dict().items()])\n",
    "        else:\n",
    "            policy_info = \"No policy data found.\"\n",
    "\n",
    "        # Prompt instructions\n",
    "        tone_instruction = random.choice(tones)\n",
    "        note_instruction = f\"{random.choice(note_styles)} {random.choice(note_scenarios)}\"\n",
    "        agent_profile = random.choice(agent_profiles)\n",
    "        claim_mood = random.choice(claim_moods)\n",
    "        claim_context = random.choice(claim_complexities)\n",
    "        instructions = f\"\"\"{tone_instruction} {agent_profile} {claim_mood} {claim_context}. \n",
    "        For notes: {note_instruction}\"\"\"\n",
    "\n",
    "        # Build LLM prompt\n",
    "        system_msg = SystemMessage(content=\"You are a claims assistant generating realistic transcripts and internal adjuster notes.\")\n",
    "        user_msg = HumanMessage(content=f\"\"\"\n",
    "            You will be given insurance claim and policy details. Your job is to:\n",
    "\n",
    "            1. Generate a FNOL call transcript between the customer and agent. {instructions}\n",
    "            2. Write adjuster notes based on the call. {note_instruction}\n",
    "\n",
    "            Claim Details:\n",
    "            {row_info}\n",
    "\n",
    "            Policy Details:\n",
    "            {policy_info}\n",
    "\n",
    "            Return a JSON object with:\n",
    "            - fnol_call: the transcript\n",
    "            - adjuster_notes: the internal summary\n",
    "            \"\"\")\n",
    "\n",
    "        result = structured_llm.invoke([system_msg, user_msg])\n",
    "        fnol = fix_smart_quotes(result.fnol_call.strip())\n",
    "        notes = result.adjuster_notes.strip()\n",
    "        fnol_calls.append(fnol)\n",
    "        adjuster_notes.append(notes)\n",
    "\n",
    "    except Exception as e:\n",
    "        fnol_calls.append(\"ERROR\")\n",
    "        adjuster_notes.append(f\"ERROR: {e}\")\n",
    "\n",
    "\n",
    "# Add results to dataframe\n",
    "df_result = df.copy()\n",
    "df_result[\"fnol_call\"] = fnol_calls\n",
    "df_result[\"adjuster_notes\"] = adjuster_notes\n",
    "\n",
    "\n",
    "\n",
    "# Save the updated CSV\n",
    "df_result.to_csv(\"C://Users//SujaySunilNagvekar//VM//GEN AI//KM//claims_with_notes.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n‚úÖ Done in {round(time() - start_time, 2)} seconds. File saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# File path\n",
    "fnol_csv = \"C://Users//SujaySunilNagvekar//VM//GEN AI//KM//claims_with_notes.csv\"\n",
    "policy_data = \"C://Users//SujaySunilNagvekar//VM//GEN AI//KM//new_policy_data.csv\"\n",
    "db_path = \"C://Users//SujaySunilNagvekar//VM//GEN AI//KM//database.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ New database created: final_insurance_db.db\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Load the cleaned CSVs\n",
    "fnol_df = pd.read_csv(fnol_csv, encoding=\"utf-8\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "policy_df = pd.read_csv(policy_data, encoding=\"utf-8\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "fnol_df.columns = fnol_df.columns.str.strip().str.replace(\"'\", \"'\").str.replace(\"-\", \"-\")\n",
    "policy_df.columns = policy_df.columns.str.strip().str.replace(\"'\", \"'\").str.replace(\"-\", \"-\")\n",
    "\n",
    "\n",
    "# Rename columns for fnol_data\n",
    "fnol_rename_map = {\n",
    "    \"Claim Number\": \"claim_number\",\n",
    "    \"Policy Number\": \"policy_number\",\n",
    "    \"Date of Loss\": \"date_of_loss\",\n",
    "    \"Time of Loss\": \"time_of_loss\",\n",
    "    \"Loss Location\": \"loss_location\",\n",
    "    \"Loss Location State\": \"loss_location_state\",\n",
    "    \"Loss Location Zipcode\": \"loss_location_zipcode\",\n",
    "    \"Date of Reporting Loss\": \"date_of_reporting_loss\",\n",
    "    \"Loss cause\": \"loss_cause\",\n",
    "    \"Claimant Name\": \"claimant_name\",\n",
    "    \"Vehicle Make\": \"vehicle_make\",\n",
    "    \"Vehicle Model\": \"vehicle_model\",\n",
    "    \"Vehicle Year\": \"vehicle_year\",\n",
    "    \"Damage Description\": \"damage_description\",\n",
    "    \"Reported By\": \"reported_by\",\n",
    "    \"Claim Status\": \"claim_status\",\n",
    "    \"Claim Assigned\": \"claim_assigned\",\n",
    "    \"Claim Handler Name\": \"claim_handler_name\",\n",
    "    \"Litigation\": \"litigation\",\n",
    "    \"Police Report\": \"police_report\",\n",
    "    \"Photos/Videos\": \"photos_videos\",\n",
    "    \"Repair Estimate presence\": \"repair_estimate_presence\",\n",
    "    \"Repair Estimate\": \"repair_estimate\",\n",
    "    \"Repair Bill\": \"repair_bill\",\n",
    "    \"Towing Receipt\": \"towing_receipt\",\n",
    "    \"Rental Receipt\": \"rental_receipt\",\n",
    "    \"Medical & Injury Documentation\": \"medical_and_injury_documentation\",\n",
    "    \"Medical Reports\": \"medical_reports\",\n",
    "    \"Hospital Records\": \"hospital_records\",\n",
    "    \"Medical bill\": \"medical_bill\",\n",
    "    \"Total Claim Bill\": \"total_claim_bill\",\n",
    "    \"Third-Party Information\": \"third_party_information\",\n",
    "    \"Subro Opportunity\": \"subro_opportunity\",\n",
    "    \"Third-Party Insurance\": \"third_party_insurance\",\n",
    "    \"Third-Party Claim Form\": \"third_party_claim_form\",\n",
    "    \"Damage Severity\": \"damage_severity\",\n",
    "    \"Injury Severity\": \"injury_severity\",\n",
    "    \"Date of Payment - Repair\": \"date_of_payment_repair\",\n",
    "    \"Date of Payment - Medical\": \"date_of_payment_medical\",\n",
    "    \"fnol_call\": \"fnol_call\",\n",
    "    \"adjuster_notes\": \"adjuster_notes\"\n",
    "}\n",
    "\n",
    "# Rename columns for policy_data\n",
    "policy_rename_map = {\n",
    "    \"Policy Number\": \"policy_number\",\n",
    "    \"Account Number\": \"account_number\",\n",
    "    \"Expiration Date\": \"expiration_date\",\n",
    "    \"Effective Date\": \"effective_date\",\n",
    "    \"Policy Status\": \"policy_status\",\n",
    "    \"Policy Term\": \"policy_term\",\n",
    "    \"Product Line\": \"product_line\",\n",
    "    \"Primary Insured Name\": \"primary_insured_name\",\n",
    "    \"Policy holder age\": \"policy_holder_age\",\n",
    "    \"Primary Insured Address\": \"primary_insured_address\",\n",
    "    \"Primary Insured location zipcode\": \"primary_insured_location_zipcode\",\n",
    "    \"Primary Insured location state\": \"primary_insured_location_state\",\n",
    "    \"Premium Amount\": \"premium_amount\",\n",
    "    \"Underwriter Name\": \"underwriter_name\",\n",
    "    \"Agent Name\": \"agent_name\",\n",
    "    \"Payment Plan\": \"payment_plan\",\n",
    "    \"Risk Type\": \"risk_type\",\n",
    "    \"Vehicle VIN\": \"vehicle_vin\",\n",
    "    \"Property Address\": \"property_address\",\n",
    "    \"Cancellation Date\": \"cancellation_date\",\n",
    "    \"Reason for Cancellation\": \"reason_for_cancellation\"\n",
    "}\n",
    "\n",
    "# Apply renaming\n",
    "fnol_df.rename(columns=fnol_rename_map, inplace=True)\n",
    "policy_df.rename(columns=policy_rename_map, inplace=True)\n",
    "\n",
    "# Create a new SQLite DB and insert tables\n",
    "conn = sqlite3.connect(\"final_insurance_db.db\")\n",
    "fnol_df.to_sql(\"fnol_data\", conn, if_exists=\"replace\", index=False)\n",
    "policy_df.to_sql(\"policy_data\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()\n",
    "\n",
    "print(\"‚úÖ New database created: final_insurance_db.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Tables in database:\n",
      "- fnol_data\n",
      "- policy_data\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"C://Users//SujaySunilNagvekar//VM\\GEN AI//KM//vm-GenAI_BI//my_database.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "print(\"üìã Tables in database:\")\n",
    "for table in tables:\n",
    "    print(\"-\", table[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßæ Columns in 'fnol_data':\n",
      "- claim_number\n",
      "- policy_number\n",
      "- date_of_loss\n",
      "- time_of_loss\n",
      "- loss_location\n",
      "- loss_location_state\n",
      "- loss_location_zipcode\n",
      "- date_of_reporting_loss\n",
      "- loss_cause\n",
      "- claimant_name\n",
      "- vehicle_make\n",
      "- vehicle_model\n",
      "- vehicle_year\n",
      "- damage_description\n",
      "- reported_by\n",
      "- claim_status\n",
      "- claim_assigned\n",
      "- claim_handler_name\n",
      "- litigation\n",
      "- police_report\n",
      "- photos_videos\n",
      "- repair_estimate_presence\n",
      "- repair_estimate\n",
      "- repair_bill\n",
      "- towing_receipt\n",
      "- rental_receipt\n",
      "- medical_and_injury_documentation\n",
      "- medical_reports\n",
      "- hospital_records\n",
      "- medical_bill\n",
      "- total_claim_bill\n",
      "- third_party_information\n",
      "- subro_opportunity\n",
      "- third_party_insurance\n",
      "- third_party_claim_form\n",
      "- damage_severity\n",
      "- injury_severity\n",
      "- date_of_payment_repair\n",
      "- date_of_payment_medical\n",
      "- fnol_call\n",
      "- adjuster_notes\n"
     ]
    }
   ],
   "source": [
    "table_name = \"fnol_data\"  # or \"policy_data\"\n",
    "cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "columns = cursor.fetchall()\n",
    "\n",
    "print(f\"\\nüßæ Columns in '{table_name}':\")\n",
    "for col in columns:\n",
    "    print(\"-\", col[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Sample repair_bill values:\n",
      "('Yes',)\n",
      "('Yes',)\n",
      "('Yes',)\n",
      "('Yes',)\n",
      "('Yes',)\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT subro_opportunity FROM fnol_data limit 5\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "print(\"\\nüîç Sample repair_bill values:\")\n",
    "for row in rows:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim_number\n",
      "policy_number\n",
      "date_of_loss\n",
      "time_of_loss\n",
      "loss_location\n",
      "loss_location_state\n",
      "loss_location_zipcode\n",
      "date_of_reporting_loss\n",
      "loss_cause\n",
      "claimant_name\n",
      "vehicle_make\n",
      "vehicle_model\n",
      "vehicle_year\n",
      "damage_description\n",
      "reported_by\n",
      "claim_status\n",
      "claim_assigned\n",
      "claim_handler_name\n",
      "Litigation \n",
      "police_report\n",
      "photos_videos\n",
      "repair_estimate_presence\n",
      "repair_estimate\n",
      "repair_bill\n",
      "towing_receipt\n",
      "rental_receipt\n",
      "medical_and_injury_documentation\n",
      "medical_reports\n",
      "hospital_records\n",
      "Medical bill\n",
      "total_claim_bill\n",
      "third_party_information\n",
      "Subro Opportunity \n",
      "third_party_insurance\n",
      "third_party_claim_form\n",
      "damage_severity\n",
      "injury_severity\n",
      "date_of_payment_repair\n",
      "date_of_payment_medical\n",
      "fnol_call\n",
      "adjuster_notes\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"PRAGMA table_info(fnol_data);\")\n",
    "columns = cursor.fetchall()\n",
    "for col in columns:\n",
    "    print(col[1])  # Column name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
